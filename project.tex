\documentclass[11pt]{article}

    \usepackage[breakable]{tcolorbox}
    \usepackage{parskip} % Stop auto-indenting (to mimic markdown behaviour)
    

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % Maintain compatibility with old templates. Remove in nbconvert 6.0
    \let\Oldincludegraphics\includegraphics
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionFormat{nocaption}{}
    \captionsetup{format=nocaption,aboveskip=0pt,belowskip=0pt}

    \usepackage{float}
    \floatplacement{figure}{H} % forces figures to be placed at the correct location
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro

    \usepackage{iftex}
    \ifPDFTeX
        \usepackage[T1]{fontenc}
        \IfFileExists{alphabeta.sty}{
              \usepackage{alphabeta}
          }{
              \usepackage[mathletters]{ucs}
              \usepackage[utf8x]{inputenc}
          }
    \else
        \usepackage{fontspec}
        \usepackage{unicode-math}
    \fi

    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics
                         % to support a larger range
    \makeatletter % fix for old versions of grffile with XeLaTeX
    \@ifpackagelater{grffile}{2019/11/01}
    {
      % Do nothing on new versions
    }
    {
      \def\Gread@@xetex#1{%
        \IfFileExists{"\Gin@base".bb}%
        {\Gread@eps{\Gin@base.bb}}%
        {\Gread@@xetex@aux#1}%
      }
    }
    \makeatother
    \usepackage[Export]{adjustbox} % Used to constrain images to a maximum size
    \adjustboxset{max size={0.9\linewidth}{0.9\paperheight}}

    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    % The default LaTeX title has an obnoxious amount of whitespace. By default,
    % titling removes some of it. It also provides customization options.
    \usepackage{titling}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage{array}     % table support for pandoc >= 2.11.3
    \usepackage{calc}      % table minipage width calculation for pandoc >= 2.11.1
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    \usepackage{mathrsfs}
    

    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}
    \definecolor{ansi-default-inverse-fg}{HTML}{FFFFFF}
    \definecolor{ansi-default-inverse-bg}{HTML}{000000}

    % common color for the border for error outputs.
    \definecolor{outerrorbackground}{HTML}{FFDFDF}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}

    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}


    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatibility definitions
    \def\gt{>}
    \def\lt{<}
    \let\Oldtex\TeX
    \let\Oldlatex\LaTeX
    \renewcommand{\TeX}{\textrm{\Oldtex}}
    \renewcommand{\LaTeX}{\textrm{\Oldlatex}}
    % Document parameters
    % Document title
    \title{project}
    
    
    
    
    
% Pygments definitions
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\@namedef{PY@tok@w}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\@namedef{PY@tok@c}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cp}{\def\PY@tc##1{\textcolor[rgb]{0.61,0.40,0.00}{##1}}}
\@namedef{PY@tok@k}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kp}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kt}{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\@namedef{PY@tok@o}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@ow}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\@namedef{PY@tok@nb}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@nf}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@nc}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@nn}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@ne}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.80,0.25,0.22}{##1}}}
\@namedef{PY@tok@nv}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@no}{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\@namedef{PY@tok@nl}{\def\PY@tc##1{\textcolor[rgb]{0.46,0.46,0.00}{##1}}}
\@namedef{PY@tok@ni}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.44,0.44,0.44}{##1}}}
\@namedef{PY@tok@na}{\def\PY@tc##1{\textcolor[rgb]{0.41,0.47,0.13}{##1}}}
\@namedef{PY@tok@nt}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@nd}{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\@namedef{PY@tok@s}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sd}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@si}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.64,0.35,0.47}{##1}}}
\@namedef{PY@tok@se}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.36,0.12}{##1}}}
\@namedef{PY@tok@sr}{\def\PY@tc##1{\textcolor[rgb]{0.64,0.35,0.47}{##1}}}
\@namedef{PY@tok@ss}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@sx}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@m}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@gh}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\@namedef{PY@tok@gu}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\@namedef{PY@tok@gd}{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\@namedef{PY@tok@gi}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.52,0.00}{##1}}}
\@namedef{PY@tok@gr}{\def\PY@tc##1{\textcolor[rgb]{0.89,0.00,0.00}{##1}}}
\@namedef{PY@tok@ge}{\let\PY@it=\textit}
\@namedef{PY@tok@gs}{\let\PY@bf=\textbf}
\@namedef{PY@tok@gp}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\@namedef{PY@tok@go}{\def\PY@tc##1{\textcolor[rgb]{0.44,0.44,0.44}{##1}}}
\@namedef{PY@tok@gt}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\@namedef{PY@tok@err}{\def\PY@bc##1{{\setlength{\fboxsep}{\string -\fboxrule}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}}
\@namedef{PY@tok@kc}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kd}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kn}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kr}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@bp}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@fm}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@vc}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vg}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vi}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vm}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@sa}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sb}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sc}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@dl}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@s2}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sh}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@s1}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@mb}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mf}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mh}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mi}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@il}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mo}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@ch}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cm}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cpf}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@c1}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cs}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % For linebreaks inside Verbatim environment from package fancyvrb.
    \makeatletter
        \newbox\Wrappedcontinuationbox
        \newbox\Wrappedvisiblespacebox
        \newcommand*\Wrappedvisiblespace {\textcolor{red}{\textvisiblespace}}
        \newcommand*\Wrappedcontinuationsymbol {\textcolor{red}{\llap{\tiny$\m@th\hookrightarrow$}}}
        \newcommand*\Wrappedcontinuationindent {3ex }
        \newcommand*\Wrappedafterbreak {\kern\Wrappedcontinuationindent\copy\Wrappedcontinuationbox}
        % Take advantage of the already applied Pygments mark-up to insert
        % potential linebreaks for TeX processing.
        %        {, <, #, %, $, ' and ": go to next line.
        %        _, }, ^, &, >, - and ~: stay at end of broken line.
        % Use of \textquotesingle for straight quote.
        \newcommand*\Wrappedbreaksatspecials {%
            \def\PYGZus{\discretionary{\char`\_}{\Wrappedafterbreak}{\char`\_}}%
            \def\PYGZob{\discretionary{}{\Wrappedafterbreak\char`\{}{\char`\{}}%
            \def\PYGZcb{\discretionary{\char`\}}{\Wrappedafterbreak}{\char`\}}}%
            \def\PYGZca{\discretionary{\char`\^}{\Wrappedafterbreak}{\char`\^}}%
            \def\PYGZam{\discretionary{\char`\&}{\Wrappedafterbreak}{\char`\&}}%
            \def\PYGZlt{\discretionary{}{\Wrappedafterbreak\char`\<}{\char`\<}}%
            \def\PYGZgt{\discretionary{\char`\>}{\Wrappedafterbreak}{\char`\>}}%
            \def\PYGZsh{\discretionary{}{\Wrappedafterbreak\char`\#}{\char`\#}}%
            \def\PYGZpc{\discretionary{}{\Wrappedafterbreak\char`\%}{\char`\%}}%
            \def\PYGZdl{\discretionary{}{\Wrappedafterbreak\char`\$}{\char`\$}}%
            \def\PYGZhy{\discretionary{\char`\-}{\Wrappedafterbreak}{\char`\-}}%
            \def\PYGZsq{\discretionary{}{\Wrappedafterbreak\textquotesingle}{\textquotesingle}}%
            \def\PYGZdq{\discretionary{}{\Wrappedafterbreak\char`\"}{\char`\"}}%
            \def\PYGZti{\discretionary{\char`\~}{\Wrappedafterbreak}{\char`\~}}%
        }
        % Some characters . , ; ? ! / are not pygmentized.
        % This macro makes them "active" and they will insert potential linebreaks
        \newcommand*\Wrappedbreaksatpunct {%
            \lccode`\~`\.\lowercase{\def~}{\discretionary{\hbox{\char`\.}}{\Wrappedafterbreak}{\hbox{\char`\.}}}%
            \lccode`\~`\,\lowercase{\def~}{\discretionary{\hbox{\char`\,}}{\Wrappedafterbreak}{\hbox{\char`\,}}}%
            \lccode`\~`\;\lowercase{\def~}{\discretionary{\hbox{\char`\;}}{\Wrappedafterbreak}{\hbox{\char`\;}}}%
            \lccode`\~`\:\lowercase{\def~}{\discretionary{\hbox{\char`\:}}{\Wrappedafterbreak}{\hbox{\char`\:}}}%
            \lccode`\~`\?\lowercase{\def~}{\discretionary{\hbox{\char`\?}}{\Wrappedafterbreak}{\hbox{\char`\?}}}%
            \lccode`\~`\!\lowercase{\def~}{\discretionary{\hbox{\char`\!}}{\Wrappedafterbreak}{\hbox{\char`\!}}}%
            \lccode`\~`\/\lowercase{\def~}{\discretionary{\hbox{\char`\/}}{\Wrappedafterbreak}{\hbox{\char`\/}}}%
            \catcode`\.\active
            \catcode`\,\active
            \catcode`\;\active
            \catcode`\:\active
            \catcode`\?\active
            \catcode`\!\active
            \catcode`\/\active
            \lccode`\~`\~
        }
    \makeatother

    \let\OriginalVerbatim=\Verbatim
    \makeatletter
    \renewcommand{\Verbatim}[1][1]{%
        %\parskip\z@skip
        \sbox\Wrappedcontinuationbox {\Wrappedcontinuationsymbol}%
        \sbox\Wrappedvisiblespacebox {\FV@SetupFont\Wrappedvisiblespace}%
        \def\FancyVerbFormatLine ##1{\hsize\linewidth
            \vtop{\raggedright\hyphenpenalty\z@\exhyphenpenalty\z@
                \doublehyphendemerits\z@\finalhyphendemerits\z@
                \strut ##1\strut}%
        }%
        % If the linebreak is at a space, the latter will be displayed as visible
        % space at end of first line, and a continuation symbol starts next line.
        % Stretch/shrink are however usually zero for typewriter font.
        \def\FV@Space {%
            \nobreak\hskip\z@ plus\fontdimen3\font minus\fontdimen4\font
            \discretionary{\copy\Wrappedvisiblespacebox}{\Wrappedafterbreak}
            {\kern\fontdimen2\font}%
        }%

        % Allow breaks at special characters using \PYG... macros.
        \Wrappedbreaksatspecials
        % Breaks at punctuation characters . , ; ? ! and / need catcode=\active
        \OriginalVerbatim[#1,codes*=\Wrappedbreaksatpunct]%
    }
    \makeatother

    % Exact colors from NB
    \definecolor{incolor}{HTML}{303F9F}
    \definecolor{outcolor}{HTML}{D84315}
    \definecolor{cellborder}{HTML}{CFCFCF}
    \definecolor{cellbackground}{HTML}{F7F7F7}

    % prompt
    \makeatletter
    \newcommand{\boxspacing}{\kern\kvtcb@left@rule\kern\kvtcb@boxsep}
    \makeatother
    \newcommand{\prompt}[4]{
        {\ttfamily\llap{{\color{#2}[#3]:\hspace{3pt}#4}}\vspace{-\baselineskip}}
    }
    

    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

\begin{document}
    
    \maketitle
    
    

    
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{11}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{multiprocessing}
\PY{k+kn}{import} \PY{n+nn}{random}
\PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
\PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}

\PY{k+kn}{from} \PY{n+nn}{tic\PYZus{}env} \PY{k+kn}{import} \PY{n}{TictactoeEnv}\PY{p}{,} \PY{n}{OptimalPlayer}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{12}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} In case of using parallel calculations}

\PY{k}{def} \PY{n+nf}{worker}\PY{p}{(}\PY{n}{func}\PY{p}{,} \PY{n}{arg}\PY{p}{,} \PY{n}{return\PYZus{}dict}\PY{p}{)}\PY{p}{:}
    \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}worker function\PYZdq{}\PYZdq{}\PYZdq{}}
    \PY{n}{return\PYZus{}dict}\PY{p}{[}\PY{n}{arg}\PY{p}{]} \PY{o}{=} \PY{n}{func}\PY{p}{(}\PY{n}{arg}\PY{p}{)}

\PY{k}{def} \PY{n+nf}{to\PYZus{}parallel}\PY{p}{(}\PY{n}{func}\PY{p}{,} \PY{n}{func\PYZus{}args}\PY{p}{)}\PY{p}{:}
    \PY{n}{manager} \PY{o}{=} \PY{n}{multiprocessing}\PY{o}{.}\PY{n}{Manager}\PY{p}{(}\PY{p}{)}
    \PY{n}{return\PYZus{}dict} \PY{o}{=} \PY{n}{manager}\PY{o}{.}\PY{n}{dict}\PY{p}{(}\PY{p}{)}
    \PY{n}{jobs} \PY{o}{=} \PY{p}{[}\PY{p}{]}
    \PY{k}{for} \PY{n}{arg} \PY{o+ow}{in} \PY{n}{func\PYZus{}args}\PY{p}{:}
        \PY{n}{p} \PY{o}{=} \PY{n}{multiprocessing}\PY{o}{.}\PY{n}{Process}\PY{p}{(}\PY{n}{target}\PY{o}{=}\PY{n}{worker}\PY{p}{,} \PY{n}{args}\PY{o}{=}\PY{p}{(}\PY{n}{func}\PY{p}{,} \PY{n}{arg}\PY{p}{,} \PY{n}{return\PYZus{}dict}\PY{p}{)}\PY{p}{)}
        \PY{n}{jobs}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{p}\PY{p}{)}
        \PY{n}{p}\PY{o}{.}\PY{n}{start}\PY{p}{(}\PY{p}{)}

    \PY{k}{for} \PY{n}{proc} \PY{o+ow}{in} \PY{n}{jobs}\PY{p}{:}
        \PY{n}{proc}\PY{o}{.}\PY{n}{join}\PY{p}{(}\PY{p}{)}
    \PY{k}{return} \PY{n+nb}{dict}\PY{p}{(}\PY{n}{return\PYZus{}dict}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \hypertarget{todo}{%
\paragraph{TODO}\label{todo}}

\begin{itemize}
\tightlist
\item
  Move to real Q-table
\item
  Generate sparse 5477x9 table and state2index dict
\item
  Use argmax in this table
\item
  Fix \texttt{update\_qtable} and \texttt{act}
\item
  Add \texttt{play\_game} function instead of \texttt{test}
\item
  Make \emph{more correct} states trace list
\item
  Plot trend line of our model
\item
  Modify ``choice of \(\epsilon\)''
\item
  Perform grid search through all decay gammas, lrs and expl rates
\item
  Check if the ``averaging the rewards'' is calculated correctly
\item
  More precise search for n*
\item
  Modify the explaination of n* effect
\item
  Why exploit is the same for all decreasing eps?
\item
  Add answer to 3rd question
\item
  Create Mrand and Mopt funcions
\item
  Create short functions for training + params
\item
  Fix a bit DQL Lfsp paragraph
\end{itemize}

    \hypertarget{q-learning}{%
\section{Q-learning}\label{q-learning}}

    As our 1st algorithm, we use Q-Learning combined with
\(\epsilon\)-greedy policy - see section \(6.5\) of Sutton and Barto
(2018) for details. At each time \(t\), state \(s_{t}\) is the board
position (showing empty positions, positions taken by you, and positions
taken by your opponent; c.f. tic\_tac\_toe.ipynb), action \(a_{t}\) is
one of the available positions on the board (i.e.~\(\epsilon\)-greedy is
applied only over the available actions), and reward \(r_{t}\) is only
non-zero when the game ends where you get \(r_{t}=1\) if you win the
game, \(r_{t}=-1\) if you lose, and \(r_{t}=0\) if it is a draw.

Q-Learning has 3 hyper-parameters: learning rate \(\alpha\), discount
factor \(\gamma\), and exploration level \(\epsilon\). For convenience,
we fix the learning rate at \(\alpha=0.05\) and the discount factor at
\(\gamma=0.99\). We initialize all the \(Q\)-values at 0 ; if you are
curious, you can explore the effect of \(\alpha, \gamma\), and initial
\(Q\)-values for yourself.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{13}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Converts state [\PYZhy{}1 1 0 0 1 \PYZhy{}1 0 \PYZhy{}1 0] to 021120101 and then to int from base 3}
\PY{n}{st2int} \PY{o}{=} \PY{k}{lambda} \PY{n}{st}\PY{p}{:} \PY{n+nb}{int}\PY{p}{(}\PY{p}{(}\PY{p}{(}\PY{n}{st}\PY{o}{.}\PY{n}{flatten}\PY{p}{(}\PY{p}{)} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{)} \PY{o}{*} \PY{p}{(}\PY{l+m+mi}{3}\PY{o}{*}\PY{o}{*}\PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{l+m+mi}{9}\PY{p}{)}\PY{p}{)}\PY{p}{)}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{p}{)}\PY{p}{)}

\PY{c+c1}{\PYZsh{} https://xkcd.com/832/}
\PY{k}{class} \PY{n+nc}{Player}\PY{p}{:}
    \PY{k}{def} \PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{learning\PYZus{}rate}\PY{p}{,} \PY{n}{decay\PYZus{}gamma}\PY{p}{,} \PY{n}{exp\PYZus{}rate}\PY{p}{)}\PY{p}{:}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{states} \PY{o}{=} \PY{p}{[}\PY{p}{]}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{states\PYZus{}value} \PY{o}{=} \PY{p}{\PYZob{}}\PY{p}{\PYZcb{}}  \PY{c+c1}{\PYZsh{} state \PYZhy{}\PYZgt{} value}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{lr} \PY{o}{=} \PY{n}{learning\PYZus{}rate}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{decay\PYZus{}gamma} \PY{o}{=} \PY{n}{decay\PYZus{}gamma}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{exp\PYZus{}rate} \PY{o}{=} \PY{n}{exp\PYZus{}rate}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{exp} \PY{o}{=} \PY{l+m+mi}{0}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{env} \PY{o}{=} \PY{n}{TictactoeEnv}\PY{p}{(}\PY{p}{)}
                
    \PY{k}{def} \PY{n+nf}{act}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{grid}\PY{p}{,} \PY{n}{symbol}\PY{p}{,} \PY{n}{eps}\PY{p}{)}\PY{p}{:}
        \PY{k}{if} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{uniform}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)} \PY{o}{\PYZlt{}}\PY{o}{=} \PY{n}{eps}\PY{p}{:}
            \PY{n}{action} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{random\PYZus{}move}\PY{p}{(}\PY{n}{grid}\PY{p}{)}
        \PY{k}{else}\PY{p}{:}
            \PY{n}{positions} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{available\PYZus{}positions}\PY{p}{(}\PY{n}{grid}\PY{p}{)}
            \PY{n}{value\PYZus{}max} \PY{o}{=} \PY{o}{\PYZhy{}}\PY{l+m+mi}{999}
            \PY{n}{num\PYZus{}symb} \PY{o}{=} \PY{l+m+mi}{1} \PY{k}{if} \PY{n}{symbol} \PY{o}{==} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{X}\PY{l+s+s1}{\PYZsq{}} \PY{k}{else} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}
            \PY{k}{for} \PY{n}{p} \PY{o+ow}{in} \PY{n}{positions}\PY{p}{:}
                \PY{n}{next\PYZus{}board} \PY{o}{=} \PY{n}{grid}\PY{o}{.}\PY{n}{copy}\PY{p}{(}\PY{p}{)}
                \PY{n}{next\PYZus{}board}\PY{p}{[}\PY{n}{p}\PY{p}{]} \PY{o}{=} \PY{n}{num\PYZus{}symb}
                \PY{n}{state\PYZus{}key} \PY{o}{=} \PY{n}{st2int}\PY{p}{(}\PY{n}{next\PYZus{}board}\PY{p}{)}
                \PY{n}{value} \PY{o}{=} \PY{l+m+mi}{0} \PY{k}{if} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{states\PYZus{}value}\PY{o}{.}\PY{n}{get}\PY{p}{(}\PY{n}{state\PYZus{}key}\PY{p}{)} \PY{o+ow}{is} \PY{k+kc}{None} \PY{k}{else} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{states\PYZus{}value}\PY{p}{[}\PY{n}{state\PYZus{}key}\PY{p}{]}
                \PY{k}{if} \PY{n}{value} \PY{o}{\PYZgt{}}\PY{o}{=} \PY{n}{value\PYZus{}max}\PY{p}{:}
                    \PY{n}{value\PYZus{}max} \PY{o}{=} \PY{n}{value}
                    \PY{n}{action} \PY{o}{=} \PY{n}{p}
        \PY{k}{return} \PY{n}{action}
    
    \PY{k}{def} \PY{n+nf}{random\PYZus{}move}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{grid}\PY{p}{)}\PY{p}{:}
        \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{} Chose a random move from the available options. \PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{k}{return} \PY{n}{random}\PY{o}{.}\PY{n}{choice}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{available\PYZus{}positions}\PY{p}{(}\PY{n}{grid}\PY{p}{)}\PY{p}{)}
         
    \PY{k}{def} \PY{n+nf}{available\PYZus{}positions}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{grid}\PY{p}{)}\PY{p}{:} \PY{c+c1}{\PYZsh{}nommé empty dans env}
        \PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{}return all empty positions\PYZsq{}\PYZsq{}\PYZsq{}}
        \PY{k}{return} \PY{p}{[}\PY{p}{(}\PY{n}{i} \PY{o}{/}\PY{o}{/} \PY{l+m+mi}{3}\PY{p}{,} \PY{n}{i} \PY{o}{\PYZpc{}} \PY{l+m+mi}{3}\PY{p}{)} \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{9}\PY{p}{)} \PY{k}{if} \PY{o+ow}{not} \PY{n}{grid}\PY{p}{[}\PY{p}{(}\PY{n}{i} \PY{o}{/}\PY{o}{/} \PY{l+m+mi}{3}\PY{p}{,} \PY{n}{i} \PY{o}{\PYZpc{}} \PY{l+m+mi}{3}\PY{p}{)}\PY{p}{]}\PY{p}{]}
    
    \PY{k}{def} \PY{n+nf}{update\PYZus{}qtable}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{states}\PY{p}{,} \PY{n}{reward}\PY{p}{)}\PY{p}{:}
        \PY{k}{for} \PY{n}{state} \PY{o+ow}{in} \PY{n+nb}{reversed}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{states}\PY{p}{)}\PY{p}{:}
            \PY{k}{if} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{states\PYZus{}value}\PY{o}{.}\PY{n}{get}\PY{p}{(}\PY{n}{state}\PY{p}{)} \PY{o+ow}{is} \PY{k+kc}{None}\PY{p}{:}
                \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{states\PYZus{}value}\PY{p}{[}\PY{n}{state}\PY{p}{]} \PY{o}{=} \PY{l+m+mi}{0}
            \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{states\PYZus{}value}\PY{p}{[}\PY{n}{state}\PY{p}{]} \PY{o}{+}\PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{lr}\PY{o}{*}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{decay\PYZus{}gamma}\PY{o}{*}\PY{n}{reward} \PY{o}{\PYZhy{}} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{states\PYZus{}value}\PY{p}{[}\PY{n}{state}\PY{p}{]}\PY{p}{)}
            \PY{n}{reward} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{states\PYZus{}value}\PY{p}{[}\PY{n}{state}\PY{p}{]}
            
    \PY{k}{def} \PY{n+nf}{add\PYZus{}state}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{state}\PY{p}{)}\PY{p}{:}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{states}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{st2int}\PY{p}{(}\PY{n}{state}\PY{p}{)}\PY{p}{)}
            
    \PY{k}{def} \PY{n+nf}{train}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{N}\PY{p}{,} \PY{n}{opponent\PYZus{}eps}\PY{p}{,} \PY{n}{exp\PYZus{}rate}\PY{o}{=}\PY{k+kc}{None}\PY{p}{)}\PY{p}{:}
        \PY{n}{exp\PYZus{}rate} \PY{o}{=} \PY{n}{exp\PYZus{}rate} \PY{k}{if} \PY{n}{exp\PYZus{}rate} \PY{o}{!=} \PY{k+kc}{None} \PY{k}{else} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{exp\PYZus{}rate}
        \PY{n}{Turns} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{X}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{O}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
        \PY{n}{random}\PY{o}{.}\PY{n}{shuffle}\PY{p}{(}\PY{n}{Turns}\PY{p}{)}
        \PY{n}{total\PYZus{}reward} \PY{o}{=} \PY{l+m+mi}{0}
        \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{N}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{:}
            \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{states} \PY{o}{=} \PY{p}{[}\PY{p}{]}
            \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{exp} \PY{o}{+}\PY{o}{=} \PY{l+m+mi}{1}
            \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{env}\PY{o}{.}\PY{n}{reset}\PY{p}{(}\PY{p}{)}
            \PY{n}{grid}\PY{p}{,} \PY{n}{\PYZus{}}\PY{p}{,} \PY{n}{\PYZus{}\PYZus{}} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{env}\PY{o}{.}\PY{n}{observe}\PY{p}{(}\PY{p}{)}
            \PY{n}{Turns} \PY{o}{=} \PY{n}{Turns}\PY{p}{[}\PY{p}{:}\PY{p}{:}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}
            \PY{n}{player\PYZus{}opt} \PY{o}{=} \PY{n}{OptimalPlayer}\PY{p}{(}\PY{n}{opponent\PYZus{}eps}\PY{p}{,} \PY{n}{player}\PY{o}{=}\PY{n}{Turns}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
                
            \PY{n}{end} \PY{o}{=} \PY{l+m+mi}{0}  \PY{c+c1}{\PYZsh{} to run first iteration}
            \PY{k}{while} \PY{o+ow}{not} \PY{n}{end}\PY{p}{:}
                \PY{k}{if} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{env}\PY{o}{.}\PY{n}{current\PYZus{}player} \PY{o}{==} \PY{n}{player\PYZus{}opt}\PY{o}{.}\PY{n}{player}\PY{p}{:}
                    \PY{n}{move} \PY{o}{=} \PY{n}{player\PYZus{}opt}\PY{o}{.}\PY{n}{act}\PY{p}{(}\PY{n}{grid}\PY{p}{)}
                    \PY{n}{grid}\PY{p}{,} \PY{n}{end}\PY{p}{,} \PY{n}{winner} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{env}\PY{o}{.}\PY{n}{step}\PY{p}{(}\PY{n}{move}\PY{p}{,} \PY{n}{print\PYZus{}grid}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}
                \PY{k}{else}\PY{p}{:}
                    \PY{n}{move} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{act}\PY{p}{(}\PY{n}{grid}\PY{p}{,} \PY{n}{Turns}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{exp\PYZus{}rate}\PY{p}{)}
                    \PY{n}{grid}\PY{p}{,} \PY{n}{end}\PY{p}{,} \PY{n}{winner} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{env}\PY{o}{.}\PY{n}{step}\PY{p}{(}\PY{n}{move}\PY{p}{,} \PY{n}{print\PYZus{}grid}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}
                    \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{add\PYZus{}state}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{env}\PY{o}{.}\PY{n}{grid}\PY{o}{.}\PY{n}{flatten}\PY{p}{(}\PY{p}{)}\PY{p}{)}
                    
            \PY{n}{reward} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{env}\PY{o}{.}\PY{n}{reward}\PY{p}{(}\PY{n}{Turns}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
            \PY{n}{total\PYZus{}reward} \PY{o}{+}\PY{o}{=} \PY{n}{reward}
            \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{update\PYZus{}qtable}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{states}\PY{p}{,} \PY{n}{reward}\PY{p}{)}
        \PY{k}{return} \PY{n}{total\PYZus{}reward} \PY{o}{/} \PY{n}{N}
                    
    \PY{k}{def} \PY{n+nf}{test}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{N}\PY{p}{,} \PY{n}{opponent\PYZus{}eps}\PY{p}{,} \PY{n}{exp\PYZus{}rate}\PY{o}{=}\PY{k+kc}{None}\PY{p}{)}\PY{p}{:}
        \PY{n}{exp\PYZus{}rate} \PY{o}{=} \PY{n}{exp\PYZus{}rate} \PY{k}{if} \PY{n}{exp\PYZus{}rate} \PY{o}{!=} \PY{k+kc}{None} \PY{k}{else} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{exp\PYZus{}rate}        
        \PY{n}{Turns} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{X}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{O}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
        \PY{n}{random}\PY{o}{.}\PY{n}{shuffle}\PY{p}{(}\PY{n}{Turns}\PY{p}{)}
        \PY{n}{total\PYZus{}reward} \PY{o}{=} \PY{l+m+mi}{0}
        \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{N}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{:}
            \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{env}\PY{o}{.}\PY{n}{reset}\PY{p}{(}\PY{p}{)}
            \PY{n}{grid}\PY{p}{,} \PY{n}{\PYZus{}}\PY{p}{,} \PY{n}{\PYZus{}\PYZus{}} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{env}\PY{o}{.}\PY{n}{observe}\PY{p}{(}\PY{p}{)}
            \PY{n}{Turns} \PY{o}{=} \PY{n}{Turns}\PY{p}{[}\PY{p}{:}\PY{p}{:}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}
            \PY{n}{player\PYZus{}opt} \PY{o}{=} \PY{n}{OptimalPlayer}\PY{p}{(}\PY{n}{opponent\PYZus{}eps}\PY{p}{,} \PY{n}{player}\PY{o}{=}\PY{n}{Turns}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
                
            \PY{n}{end} \PY{o}{=} \PY{l+m+mi}{0}  \PY{c+c1}{\PYZsh{} to run first iteration}
            \PY{k}{while} \PY{o+ow}{not} \PY{n}{end}\PY{p}{:}
                \PY{k}{if} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{env}\PY{o}{.}\PY{n}{current\PYZus{}player} \PY{o}{==} \PY{n}{player\PYZus{}opt}\PY{o}{.}\PY{n}{player}\PY{p}{:}
                    \PY{n}{move} \PY{o}{=} \PY{n}{player\PYZus{}opt}\PY{o}{.}\PY{n}{act}\PY{p}{(}\PY{n}{grid}\PY{p}{)}
                    \PY{n}{grid}\PY{p}{,} \PY{n}{end}\PY{p}{,} \PY{n}{winner} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{env}\PY{o}{.}\PY{n}{step}\PY{p}{(}\PY{n}{move}\PY{p}{,} \PY{n}{print\PYZus{}grid}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}
                \PY{k}{else}\PY{p}{:}
                    \PY{n}{move} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{act}\PY{p}{(}\PY{n}{grid}\PY{p}{,} \PY{n}{Turns}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{exp\PYZus{}rate}\PY{p}{)}
                    \PY{n}{grid}\PY{p}{,} \PY{n}{end}\PY{p}{,} \PY{n}{winner} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{env}\PY{o}{.}\PY{n}{step}\PY{p}{(}\PY{n}{move}\PY{p}{,} \PY{n}{print\PYZus{}grid}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}
                    
            \PY{n}{reward} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{env}\PY{o}{.}\PY{n}{reward}\PY{p}{(}\PY{n}{Turns}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
            \PY{n}{total\PYZus{}reward} \PY{o}{+}\PY{o}{=} \PY{n}{reward} 
            
        \PY{k}{return} \PY{n}{total\PYZus{}reward} \PY{o}{/} \PY{n}{N}
\end{Verbatim}
\end{tcolorbox}

    \hypertarget{learning-from-experts}{%
\subsection{Learning from experts}\label{learning-from-experts}}

    In this section, you will study whether Q-learning can learn to play Tic
Tac Toe by playing against Opt \(\left(\epsilon_{\mathrm{opt}}\right)\)
for some \(\epsilon_{\mathrm{opt}} \in[0,1]\). To do so, implement the
\(Q\)-learning algorithm. To check the algorithm, run a \(Q\)-learning
agent, with a fixed and arbitrary \(\epsilon \in[0,1)\), against 0pt
(0.5) for 20 '000 games - switch the 1st player after every game.

    \textbf{Question 1}. Plot average reward for every 250 games during
training -- i.e.~after the 50th game, plot the average reward of the
first 250 games, after the 100th game, plot the average reward of games
51 to 100, etc. Does the agent learn to play Tic Tac Toe?\\
\emph{Expected answer}: A figure of average reward over time (caption
length \textless{} 50 words). Specify your choice of \(\epsilon\).

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{14}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{learning\PYZus{}rate} \PY{o}{=} \PY{l+m+mf}{0.05}
\PY{n}{discount\PYZus{}factor} \PY{o}{=} \PY{l+m+mf}{0.99}
\PY{n}{my\PYZus{}exp\PYZus{}rate} \PY{o}{=} \PY{l+m+mf}{0.5}
\PY{n}{opponent\PYZus{}exp\PYZus{}rate} \PY{o}{=} \PY{l+m+mf}{0.5}

\PY{n}{my\PYZus{}player} \PY{o}{=} \PY{n}{Player}\PY{p}{(}\PY{n}{learning\PYZus{}rate}\PY{p}{,} \PY{n}{discount\PYZus{}factor}\PY{p}{,} \PY{n}{my\PYZus{}exp\PYZus{}rate}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{15}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} avgs = []}
\PY{c+c1}{\PYZsh{} opponent\PYZus{}exp\PYZus{}rate = 1}
\PY{c+c1}{\PYZsh{} N = 20000}
\PY{c+c1}{\PYZsh{} avg\PYZus{}every = 250}
\PY{c+c1}{\PYZsh{} batch\PYZus{}sz = 50}
\PY{c+c1}{\PYZsh{} for i in range(1, N // avg\PYZus{}every + 1):}
\PY{c+c1}{\PYZsh{}     batch = [my\PYZus{}player.train(batch\PYZus{}sz, opponent\PYZus{}exp\PYZus{}rate) for \PYZus{} in range(avg\PYZus{}every // batch\PYZus{}sz)]}
\PY{c+c1}{\PYZsh{}     avgs.append(sum(batch) / len(batch))}
\PY{c+c1}{\PYZsh{}     print(f\PYZdq{}Ep: \PYZob{}avg\PYZus{}every*i\PYZcb{}, avg: \PYZob{}round(avgs[\PYZhy{}1], 3)\PYZcb{}, \PYZob{}batch\PYZcb{}\PYZdq{})}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{16}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{opponent\PYZus{}exp\PYZus{}rate} \PY{o}{=} \PY{l+m+mf}{0.5}
\PY{n}{N} \PY{o}{=} \PY{l+m+mi}{20000}
\PY{n}{train\PYZus{}avg\PYZus{}every} \PY{o}{=} \PY{n}{batch\PYZus{}sz} \PY{o}{=} \PY{l+m+mi}{250}
\PY{n}{test\PYZus{}avg\PYZus{}every} \PY{o}{=} \PY{l+m+mi}{1000}
\PY{n}{minibatch} \PY{o}{=} \PY{l+m+mi}{50}

\PY{n}{train} \PY{o}{=} \PY{p}{[}\PY{p}{]}
\PY{n}{test} \PY{o}{=} \PY{p}{[}\PY{p}{]}
\PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{N}\PY{p}{,} \PY{n}{train\PYZus{}avg\PYZus{}every}\PY{p}{)}\PY{p}{:}
    \PY{n}{batch} \PY{o}{=} \PY{p}{[}
        \PY{n}{my\PYZus{}player}\PY{o}{.}\PY{n}{train}\PY{p}{(}\PY{n}{minibatch}\PY{p}{,} \PY{n}{opponent\PYZus{}exp\PYZus{}rate}\PY{p}{)}
        \PY{k}{for} \PY{n}{j} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{batch\PYZus{}sz}\PY{p}{,} \PY{n}{minibatch}\PY{p}{)}
    \PY{p}{]}
    \PY{n}{train}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n+nb}{sum}\PY{p}{(}\PY{n}{batch}\PY{p}{)} \PY{o}{/} \PY{n+nb}{len}\PY{p}{(}\PY{n}{batch}\PY{p}{)}\PY{p}{)}

    \PY{k}{if} \PY{n}{i} \PY{o+ow}{and} \PY{o+ow}{not} \PY{n}{i} \PY{o}{\PYZpc{}} \PY{n}{test\PYZus{}avg\PYZus{}every}\PY{p}{:}
        \PY{n}{batch} \PY{o}{=} \PY{p}{[}
            \PY{n}{my\PYZus{}player}\PY{o}{.}\PY{n}{test}\PY{p}{(}\PY{n}{minibatch}\PY{p}{,} \PY{n}{opponent\PYZus{}exp\PYZus{}rate}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{)}
            \PY{k}{for} \PY{n}{j} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{batch\PYZus{}sz}\PY{p}{,} \PY{n}{minibatch}\PY{p}{)}
        \PY{p}{]}
        \PY{n}{test}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n+nb}{sum}\PY{p}{(}\PY{n}{batch}\PY{p}{)} \PY{o}{/} \PY{n+nb}{len}\PY{p}{(}\PY{n}{batch}\PY{p}{)}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Ep: }\PY{l+s+si}{\PYZob{}}\PY{n}{i}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{, avg: }\PY{l+s+si}{\PYZob{}}\PY{n+nb}{round}\PY{p}{(}\PY{n}{test}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        
\PY{n}{batch} \PY{o}{=} \PY{p}{[}
    \PY{n}{my\PYZus{}player}\PY{o}{.}\PY{n}{test}\PY{p}{(}\PY{n}{batch\PYZus{}sz}\PY{p}{,} \PY{n}{opponent\PYZus{}exp\PYZus{}rate}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{)}
    \PY{k}{for} \PY{n}{j} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{batch\PYZus{}sz}\PY{p}{,} \PY{n}{minibatch}\PY{p}{)}
\PY{p}{]}
\PY{n}{test}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n+nb}{sum}\PY{p}{(}\PY{n}{batch}\PY{p}{)} \PY{o}{/} \PY{n+nb}{len}\PY{p}{(}\PY{n}{batch}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Ep: }\PY{l+s+si}{\PYZob{}}\PY{n}{i}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{, avg: }\PY{l+s+si}{\PYZob{}}\PY{n+nb}{round}\PY{p}{(}\PY{n}{test}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Ep: 1000, avg: 0.024
Ep: 2000, avg: 0.352
Ep: 3000, avg: 0.44
Ep: 4000, avg: 0.532
Ep: 5000, avg: 0.556
Ep: 6000, avg: 0.48
Ep: 7000, avg: 0.556
Ep: 8000, avg: 0.516
Ep: 9000, avg: 0.54
Ep: 10000, avg: 0.544
Ep: 11000, avg: 0.628
Ep: 12000, avg: 0.604
Ep: 13000, avg: 0.616
Ep: 14000, avg: 0.6
Ep: 15000, avg: 0.628
Ep: 16000, avg: 0.584
Ep: 17000, avg: 0.588
Ep: 18000, avg: 0.644
Ep: 19000, avg: 0.66
Ep: 19750, avg: 0.65
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{17}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{train}\PY{p}{)}\PY{p}{)}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{)}\PY{o}{*}\PY{n}{train\PYZus{}avg\PYZus{}every}\PY{p}{,} \PY{n}{train}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZhy{}\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{const exp rate, train}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{test}\PY{p}{)}\PY{p}{)}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{)}\PY{o}{*}\PY{n}{test\PYZus{}avg\PYZus{}every}\PY{p}{,} \PY{n}{test}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{const exp rate, test}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}    

\PY{n}{plt}\PY{o}{.}\PY{n}{rcParams}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{figure.figsize}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]} \PY{o}{=} \PY{p}{(}\PY{l+m+mi}{16}\PY{p}{,}\PY{l+m+mi}{10}\PY{p}{)}

\PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{n}{loc}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{lower right}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Average score progression over games}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Number of games}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Average reward}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{project_files/project_12_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    The used exploration rate is \(\epsilon=0.5\) because it should half of
games explore and half of games play best strategy

    \hypertarget{decreasing-exploration}{%
\subsubsection{Decreasing exploration}\label{decreasing-exploration}}

    One way to make training more efficient is to decrease the exploration
level \(\epsilon\) over time. If we define \(\epsilon(n)\) to be
\(\epsilon\) for game number \(n\), then one feasible way to decrease
exploration during training is to use

\[
\epsilon(n)=\max \left\{\epsilon_{\min }, \epsilon_{\max }\left(1-\frac{n}{n^{*}}\right)\right\}
\]

where \(\epsilon_{\min }\) and \(\epsilon_{\max }\) are the minimum and
maximum values for \(\epsilon\), respectively, and \(n^{*}\) is the
number of exploratory games and shows how fast \(\epsilon\) decreases.
For convenience, we assume \(\epsilon_{\min }=0.1\) and
\(\epsilon_{\max }=0.8\); if you are curious, you can explore their
effect on performance for yourself. Use \(\epsilon(n)\) as define above
and run different Q-learning agents with different values of \(n^{*}\)
against Opt (0.5) for \(20^{\prime} 000\) games - switch the 1 st player
after every game. Choose several values of \(n^{*}\) from a reasonably
wide interval between 1 to \(40^{\prime} 000-\) particularly, include
\(n^{*}=1\).

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{18}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{eps\PYZus{}min}\PY{p}{,} \PY{n}{eps\PYZus{}max} \PY{o}{=} \PY{l+m+mf}{0.1}\PY{p}{,} \PY{l+m+mf}{0.8}
\PY{n}{e} \PY{o}{=} \PY{k}{lambda} \PY{n}{n}\PY{p}{,} \PY{n}{n\PYZus{}star}\PY{p}{:} \PY{n+nb}{max}\PY{p}{(}\PY{n}{eps\PYZus{}min}\PY{p}{,} \PY{n}{eps\PYZus{}max}\PY{o}{*}\PY{p}{(}\PY{l+m+mi}{1} \PY{o}{\PYZhy{}} \PY{n}{n}\PY{o}{/}\PY{n}{n\PYZus{}star}\PY{p}{)}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Eps visualization}
\PY{n}{x} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{10000}\PY{p}{,} \PY{l+m+mi}{100}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{n}{e}\PY{p}{(}\PY{n}{xi}\PY{p}{,} \PY{l+m+mi}{10000}\PY{p}{)} \PY{k}{for} \PY{n}{xi} \PY{o+ow}{in} \PY{n}{x}\PY{p}{]}\PY{p}{)}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Exploration rate over time for n*=10000}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Number of games}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{e(n)}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{project_files/project_16_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \textbf{Question 2}. Plot average reward for every 250 games during
training. Does decreasing \(\epsilon\) help training compared to having
a fixed \(\epsilon\)? What is the effect of \(\textit n^∗\)?\\
\emph{Expected answer}: A figure showing average reward over time for
different values of \(\textit n^∗\) (caption length \textless{} 200
words).

    \textbf{Question 3}. After every 250 games during training, compute the
`test' \(M_{\text {opt }}\) and \(M_{\text {rand }}\) for your agents -
when measuring the `test' performance, put \(\epsilon=0\) and do not
update the \(Q\)-values. Plot \(M_{\text {opt }}\) and
\(M_{\text {rand }}\) over time. Describe the differences and the
similarities between these curves and the ones of the previous
question.\\
\emph{Expected answer}: A figure showing \(M_{\mathrm{opt}}\) and
\(M_{\text {rand }}\) over time for different values of \(n^{*}\)
(caption length \(<100\) words).

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{19}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{n\PYZus{}stars} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{40000}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{)}\PY{o}{.}\PY{n}{astype}\PY{p}{(}\PY{n+nb}{int}\PY{p}{)}\PY{p}{;} \PY{n}{n\PYZus{}stars}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{19}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
array([    1, 10000, 20000, 30000, 40000])
\end{Verbatim}
\end{tcolorbox}
        
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{21}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{learning\PYZus{}rate} \PY{o}{=} \PY{l+m+mf}{0.05}
\PY{n}{discount\PYZus{}factor} \PY{o}{=} \PY{l+m+mf}{0.99}
\PY{n}{my\PYZus{}exp\PYZus{}rate} \PY{o}{=} \PY{l+m+mf}{0.5}

\PY{n}{opponent\PYZus{}exp\PYZus{}rate} \PY{o}{=} \PY{l+m+mf}{0.5}
\PY{n}{N} \PY{o}{=} \PY{l+m+mi}{20000}
\PY{n}{train\PYZus{}avg\PYZus{}every} \PY{o}{=} \PY{n}{batch\PYZus{}sz} \PY{o}{=} \PY{l+m+mi}{250}
\PY{n}{test\PYZus{}avg\PYZus{}every} \PY{o}{=} \PY{l+m+mi}{1000}
\PY{n}{minibatch} \PY{o}{=} \PY{l+m+mi}{50}


\PY{k}{def} \PY{n+nf}{get\PYZus{}avgs}\PY{p}{(}\PY{n}{n\PYZus{}star}\PY{p}{)}\PY{p}{:}
    \PY{n}{player} \PY{o}{=} \PY{n}{Player}\PY{p}{(}\PY{n}{learning\PYZus{}rate}\PY{p}{,} \PY{n}{discount\PYZus{}factor}\PY{p}{,} \PY{n}{my\PYZus{}exp\PYZus{}rate}\PY{p}{)}
    
    \PY{n}{avgs} \PY{o}{=} \PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{train}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{p}{]}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{test}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{p}{]}\PY{p}{\PYZcb{}}
    \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{N}\PY{p}{,} \PY{n}{train\PYZus{}avg\PYZus{}every}\PY{p}{)}\PY{p}{:}
        \PY{n}{batch} \PY{o}{=} \PY{p}{[}
            \PY{n}{player}\PY{o}{.}\PY{n}{train}\PY{p}{(}\PY{n}{minibatch}\PY{p}{,} \PY{n}{opponent\PYZus{}exp\PYZus{}rate}\PY{p}{,} \PY{n}{e}\PY{p}{(}\PY{n}{i}\PY{o}{+}\PY{n}{j}\PY{p}{,} \PY{n}{n\PYZus{}star}\PY{p}{)}\PY{p}{)}
            \PY{k}{for} \PY{n}{j} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{batch\PYZus{}sz}\PY{p}{,} \PY{n}{minibatch}\PY{p}{)}
        \PY{p}{]}
        \PY{n}{avgs}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{train}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n+nb}{sum}\PY{p}{(}\PY{n}{batch}\PY{p}{)} \PY{o}{/} \PY{n+nb}{len}\PY{p}{(}\PY{n}{batch}\PY{p}{)}\PY{p}{)}
        
        \PY{k}{if} \PY{n}{i} \PY{o+ow}{and} \PY{o+ow}{not} \PY{n}{i} \PY{o}{\PYZpc{}} \PY{n}{test\PYZus{}avg\PYZus{}every}\PY{p}{:}
            \PY{n}{batch} \PY{o}{=} \PY{p}{[}
                \PY{n}{player}\PY{o}{.}\PY{n}{test}\PY{p}{(}\PY{n}{minibatch}\PY{p}{,} \PY{n}{opponent\PYZus{}exp\PYZus{}rate}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{)}
                \PY{k}{for} \PY{n}{j} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{batch\PYZus{}sz}\PY{p}{,} \PY{n}{minibatch}\PY{p}{)}
            \PY{p}{]}
            \PY{n}{avgs}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{test}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n+nb}{sum}\PY{p}{(}\PY{n}{batch}\PY{p}{)} \PY{o}{/} \PY{n+nb}{len}\PY{p}{(}\PY{n}{batch}\PY{p}{)}\PY{p}{)}
            \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Ep: }\PY{l+s+si}{\PYZob{}}\PY{n}{i}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{, n: }\PY{l+s+si}{\PYZob{}}\PY{n}{n\PYZus{}star}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{, avg: }\PY{l+s+si}{\PYZob{}}\PY{n+nb}{round}\PY{p}{(}\PY{n}{avgs}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{test}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
 
    \PY{k}{del} \PY{n}{player}
    \PY{k}{return} \PY{n}{avgs}

\PY{c+c1}{\PYZsh{} Processing parallelizing}
\PY{c+c1}{\PYZsh{} results = to\PYZus{}parallel(get\PYZus{}avgs, n\PYZus{}stars)}
\PY{n}{results} \PY{o}{=} \PY{p}{\PYZob{}}\PY{p}{\PYZcb{}}
\PY{k}{for} \PY{n}{n\PYZus{}star} \PY{o+ow}{in} \PY{n}{n\PYZus{}stars}\PY{p}{:}
    \PY{n}{results}\PY{p}{[}\PY{n}{n\PYZus{}star}\PY{p}{]} \PY{o}{=} \PY{n}{get\PYZus{}avgs}\PY{p}{(}\PY{n}{n\PYZus{}star}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Ep: 1000, n: 1, avg: 0.224
Ep: 2000, n: 1, avg: 0.384
Ep: 3000, n: 1, avg: 0.364
Ep: 4000, n: 1, avg: 0.496
Ep: 5000, n: 1, avg: 0.484
Ep: 6000, n: 1, avg: 0.496
Ep: 7000, n: 1, avg: 0.444
Ep: 8000, n: 1, avg: 0.512
Ep: 9000, n: 1, avg: 0.52
Ep: 10000, n: 1, avg: 0.588
Ep: 11000, n: 1, avg: 0.488
Ep: 12000, n: 1, avg: 0.612
Ep: 13000, n: 1, avg: 0.532
Ep: 14000, n: 1, avg: 0.508
Ep: 15000, n: 1, avg: 0.524
Ep: 16000, n: 1, avg: 0.56
Ep: 17000, n: 1, avg: 0.588
Ep: 18000, n: 1, avg: 0.568
Ep: 19000, n: 1, avg: 0.6
Ep: 1000, n: 10000, avg: 0.08
Ep: 2000, n: 10000, avg: 0.276
Ep: 3000, n: 10000, avg: 0.304
Ep: 4000, n: 10000, avg: 0.424
Ep: 5000, n: 10000, avg: 0.516
Ep: 6000, n: 10000, avg: 0.504
Ep: 7000, n: 10000, avg: 0.616
Ep: 8000, n: 10000, avg: 0.6
Ep: 9000, n: 10000, avg: 0.56
Ep: 10000, n: 10000, avg: 0.528
Ep: 11000, n: 10000, avg: 0.596
Ep: 12000, n: 10000, avg: 0.624
Ep: 13000, n: 10000, avg: 0.64
Ep: 14000, n: 10000, avg: 0.576
Ep: 15000, n: 10000, avg: 0.544
Ep: 16000, n: 10000, avg: 0.624
Ep: 17000, n: 10000, avg: 0.556
Ep: 18000, n: 10000, avg: 0.656
Ep: 19000, n: 10000, avg: 0.632
Ep: 1000, n: 20000, avg: -0.156
Ep: 2000, n: 20000, avg: 0.1
Ep: 3000, n: 20000, avg: 0.2
Ep: 4000, n: 20000, avg: 0.464
Ep: 5000, n: 20000, avg: 0.424
Ep: 6000, n: 20000, avg: 0.536
Ep: 7000, n: 20000, avg: 0.576
Ep: 8000, n: 20000, avg: 0.576
Ep: 9000, n: 20000, avg: 0.56
Ep: 10000, n: 20000, avg: 0.548
Ep: 11000, n: 20000, avg: 0.66
Ep: 12000, n: 20000, avg: 0.612
Ep: 13000, n: 20000, avg: 0.588
Ep: 14000, n: 20000, avg: 0.6
Ep: 15000, n: 20000, avg: 0.62
Ep: 16000, n: 20000, avg: 0.612
Ep: 17000, n: 20000, avg: 0.624
Ep: 18000, n: 20000, avg: 0.596
Ep: 19000, n: 20000, avg: 0.632
Ep: 1000, n: 30000, avg: 0.072
Ep: 2000, n: 30000, avg: 0.248
Ep: 3000, n: 30000, avg: 0.272
Ep: 4000, n: 30000, avg: 0.352
Ep: 5000, n: 30000, avg: 0.396
Ep: 6000, n: 30000, avg: 0.5
Ep: 7000, n: 30000, avg: 0.572
Ep: 8000, n: 30000, avg: 0.604
Ep: 9000, n: 30000, avg: 0.54
Ep: 10000, n: 30000, avg: 0.624
Ep: 11000, n: 30000, avg: 0.6
Ep: 12000, n: 30000, avg: 0.596
Ep: 13000, n: 30000, avg: 0.572
Ep: 14000, n: 30000, avg: 0.604
Ep: 15000, n: 30000, avg: 0.604
Ep: 16000, n: 30000, avg: 0.644
Ep: 17000, n: 30000, avg: 0.624
Ep: 18000, n: 30000, avg: 0.656
Ep: 19000, n: 30000, avg: 0.648
Ep: 1000, n: 40000, avg: 0.104
Ep: 2000, n: 40000, avg: 0.104
Ep: 3000, n: 40000, avg: 0.192
Ep: 4000, n: 40000, avg: 0.428
Ep: 5000, n: 40000, avg: 0.488
Ep: 6000, n: 40000, avg: 0.56
Ep: 7000, n: 40000, avg: 0.468
Ep: 8000, n: 40000, avg: 0.568
Ep: 9000, n: 40000, avg: 0.56
Ep: 10000, n: 40000, avg: 0.616
Ep: 11000, n: 40000, avg: 0.652
Ep: 12000, n: 40000, avg: 0.612
Ep: 13000, n: 40000, avg: 0.572
Ep: 14000, n: 40000, avg: 0.592
Ep: 15000, n: 40000, avg: 0.552
Ep: 16000, n: 40000, avg: 0.54
Ep: 17000, n: 40000, avg: 0.608
Ep: 18000, n: 40000, avg: 0.724
Ep: 19000, n: 40000, avg: 0.62
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{27}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n+nb}{len}\PY{p}{(}\PY{n}{results}\PY{p}{[}\PY{n}{n}\PY{p}{]}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{test}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{27}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
19
\end{Verbatim}
\end{tcolorbox}
        
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{38}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{x\PYZus{}train} \PY{o}{=} \PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{n}{N} \PY{o}{/}\PY{o}{/} \PY{n}{train\PYZus{}avg\PYZus{}every}\PY{p}{)}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{)} \PY{o}{*} \PY{n}{train\PYZus{}avg\PYZus{}every}
\PY{n}{x\PYZus{}test} \PY{o}{=} \PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{n}{N} \PY{o}{/}\PY{o}{/} \PY{n}{test\PYZus{}avg\PYZus{}every}\PY{p}{)}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{[}\PY{p}{:}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]} \PY{o}{*} \PY{n}{test\PYZus{}avg\PYZus{}every}
\PY{k}{for} \PY{n}{n} \PY{o+ow}{in} \PY{n}{results}\PY{p}{:}
    \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{x\PYZus{}train}\PY{p}{,} \PY{n}{results}\PY{p}{[}\PY{n}{n}\PY{p}{]}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{train}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZhy{}\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+si}{\PYZob{}}\PY{n}{n}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ train}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{x\PYZus{}test}\PY{p}{,} \PY{n}{results}\PY{p}{[}\PY{n}{n}\PY{p}{]}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{test}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+si}{\PYZob{}}\PY{n}{n}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ test}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{train}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZhy{}\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{eps = const}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{x\PYZus{}test}\PY{p}{,} \PY{n}{test}\PY{p}{[}\PY{p}{:}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{eps = const}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

\PY{n}{plt}\PY{o}{.}\PY{n}{rcParams}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{figure.figsize}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]} \PY{o}{=} \PY{p}{(}\PY{l+m+mi}{20}\PY{p}{,}\PY{l+m+mi}{20}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{n}{loc}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{lower right}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Success measurement on train}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Number of games}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{exploration Mopt}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{project_files/project_22_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    It's fair to conclude regarding different \(n^*\) that: - eps = const
--- slowest learning, not the best results - n* = 1 learns fast - n* =
10k learns slower, but has results better that n* = 1 and one of the
best models - n* = 20k constant growing and best results at the end - n*
= 30k learns slower, results aren't the best - n* = 40k worst model so
far, slowest learning

    \hypertarget{good-experts-and-bad-experts}{%
\section{Good experts and bad
experts}\label{good-experts-and-bad-experts}}

    Choose the best value of \(n^{*}\) that you found in the previous
section. Run \(Q\)-learning against Opt ( \(\epsilon_{\text {opt }}\) )
for different values of \(\epsilon_{\mathrm{opt}}\) for
\(20^{\prime} 000\) games - switch the 1st player after every game.
Choose several values of \(\epsilon_{\mathrm{opt}}\) from a reasonably
wide interval between 0 to \(1-\) particularly, include
\(\epsilon_{\mathrm{opt}}=0\).

    \textbf{Question 4}. After every 250 games during training, compute the
`test' \(M_{\text {opt }}\) and \(M_{\text {rand }}\) for your agents -
for each value of \(\epsilon_{\mathrm{opt}}\). Plot \(M_{\mathrm{opt}}\)
and \(M_{\text {rand }}\) over time. What do you observe? How can you
explain it? Expected answer: A figure showing \(M_{\mathrm{opt}}\) and
\(M_{\text {rand }}\) over time for different values of
\(\epsilon_{\mathrm{opt}}\) (caption length \textless{} 250 words).

    \textbf{Question 5}. What are the highest values of \(M_{\mathrm{opt}}\)
and \(M_{\text {rand }}\) that you could achieve after playing 20 '000
games?

    \textbf{Question 6}. (Theory) Assume that Agent 1 learns by playing
against Opt ( 0 ) and find the optimal Qvalues \(Q_{1}(s, a)\). In
addition, assume that Agent 2 learns by playing against Opt (1) and find
the optimal \(Q\)-values \(Q_{2}(s, a)\). Do \(Q_{1}(s, a)\) and
\(Q_{2}(s, a)\) have the same values? Justify your answer. (answer
length \(<150\) words)

    \hypertarget{learning-by-self-practice}{%
\subsection{Learning by self-practice}\label{learning-by-self-practice}}

    In this section, your are supposed to ask whether Q-learning can learn
to play Tic Tac Toe by only playing against itself. For different values
of \(\epsilon \in[0,1)\), run a \(Q\)-learning agent against itself for
20 '000 games - i.e.~both players use the same set of \(Q\)-values and
update the same set of \(Q\)-values.

\textbf{Question 7}. After every 250 games during training, compute the
`test' \(M_{\mathrm{opt}}\) and \(M_{\text {rand }}\) for different
values of \(\epsilon \in[0,1)\). Does the agent learn to play Tic Tac
Toe? What is the effect of \(\epsilon\) ?\\
\emph{Expected answer}: A figure showing \(M_{\text {opt }}\) and
\(M_{\text {rand}}\) over time for different values of
\(\epsilon \in[0,1)\) (caption length \(<100\) words).

For rest of this section, use \(\epsilon(n)\) in Equation 1 with
different values of \(n^{*}-\) instead of fixing \(\epsilon\).

\textbf{Question 8}. After every 250 games during training, compute the
`test' \(M_{\text {opt }}\) and \(M_{\text {rand }}\) for your agents.
Does decreasing \(\epsilon\) help training compared to having a fixed
\(\epsilon\) ? What is the effect of \(n^{*}\) ?\\
\emph{Expected answer}: A figure showing \(M_{\mathrm{opt}}\) and
\(M_{\text {rand }}\) over time for different values of speeds of
\(n^{*}\) (caption length \(<100\) words) .

\textbf{Question 9}. What are the highest values of \(M_{\mathrm{opt}}\)
and \(M_{\text {rand }}\) that you could achieve after playing 20 '000
games?

\textbf{Question 10}. For three board arrangements (i.e.~states \(s\) ),
visualize Q-values of available actions (e.g.~using heat maps). Does the
result make sense? Did the agent learn the game well?\\
\emph{Expected answer}: A figure with 3 subplots of 3 different states
with \(Q\)-values shown at available actions (caption length \(<200\)
words).

    \hypertarget{deep-q-learning}{%
\section{Deep Q-Learning}\label{deep-q-learning}}

    As our 2nd algorithm, we use Deep Q-Learning (DQN) combined with
\(\epsilon\)-greedy policy. You can watch again Part 1 of Deep
Reinforcement Learning Lecture 1 for an introduction to DQN and Part 1
of Deep Reinforcement Learning Lecture 2 (in particular slide 8 ) for
more details. The idea in DQN is to approximate \(Q\)-values by a neural
network instead of a look-up table as in Tabular \(Q\)-learning. For
implementation, you can use ideas from the DQN tutorials of Keras and
PyTorch.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} import numpy as np}
\PY{c+c1}{\PYZsh{} import torch}
\PY{c+c1}{\PYZsh{} import torch.nn as nn}
\PY{c+c1}{\PYZsh{} import torch.nn.functional as F}
\PY{c+c1}{\PYZsh{} from torch.utils.data import DataLoader}
\PY{c+c1}{\PYZsh{} from torchvision.datasets import MNIST}
\PY{c+c1}{\PYZsh{} from torchvision.transforms import ToTensor}


\PY{c+c1}{\PYZsh{} class DQLNN(nn.Module):}
\PY{c+c1}{\PYZsh{}     \PYZdq{}\PYZdq{}\PYZdq{} DQLNN, expects input shape (3, 3, 2) \PYZdq{}\PYZdq{}\PYZdq{}}
\PY{c+c1}{\PYZsh{}     def \PYZus{}\PYZus{}init\PYZus{}\PYZus{}(self):}
\PY{c+c1}{\PYZsh{}         super(DQLNN, self).\PYZus{}\PYZus{}init\PYZus{}\PYZus{}()}

\PY{c+c1}{\PYZsh{}         self.fc1 = nn.Linear(3*3*2, 128)}
\PY{c+c1}{\PYZsh{}         self.fc2 = nn.Linear(128, 128)}
\PY{c+c1}{\PYZsh{}         self.fc3 = nn.Linear(128, 9)}
        
\PY{c+c1}{\PYZsh{}     def forward(self, x):}
\PY{c+c1}{\PYZsh{}         return self.fc3(F.relu(self.fc2(F.relu(self.fc1(x.flatten())))))}
    
\PY{c+c1}{\PYZsh{} model\PYZus{}dql = DQLNN()}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Player reinitializing}
\end{Verbatim}
\end{tcolorbox}

    \hypertarget{learning-from-experts}{%
\subsection{Learning from experts}\label{learning-from-experts}}

    Implement the DQN algorithm. To check the algorithm, run a DQN agent
with a fixed and arbitrary \(\epsilon \in[0,1)\) against Opt \((0.5)\)
for \(20^{\prime} 000\) games - switch the 1st player after every game.

\textbf{Question 11}. Plot average reward and average training loss for
every 250 games during training. Does the loss decrease? Does the agent
learn to play Tic Tac Toe?\\
\emph{Expected answer}: A figure with two subplots (caption length
\(<50\) words). Specify your choice of \(\epsilon\).

\textbf{Question 12}. Repeat the training but without the replay buffer
and with a batch size of 1 : At every step, update the network by using
only the latest transition. What do you observe?\\
\emph{Expected answer}: A figure with two subplots showing average
reward and average training loss during training (caption length \(<50\)
words).

Instead of fixing \(\epsilon\), use \(\epsilon(n)\) in Equation 1. For
different values of \(n^{*}\), run your DQN against Opt ( \(0.5\) ) for
20'000 games - switch the 1st player after every game. Choose several
values of \(n^{*}\) from a reasonably wide interval between 1 to
\(40^{\prime} 000\) - particularly, include \(n^{*}=1\).

\textbf{Question 13}. After every 250 games during training, compute the
`test' \(M_{\text {opt }}\) and \(M_{\text {rand }}\) for your agents.
Plot \(M_{\text {opt }}\) and \(M_{\text {rand }}\) over time. Does
decreasing \(\epsilon\) help training compared to having a fixed
\(\epsilon\) ? What is the effect of \(n^{*}\) ?\\
\emph{Expected answer}: A figure showing \(M_{\mathrm{opt}}\) and
\(M_{\text {rand }}\) over time for different values of opt (caption
length \(<250\) words).

Choose the best value of \(n^{*}\) that you found. Run DQN against Opt (
\(\epsilon_{\mathrm{opt}}\) ) for different values of
\(\epsilon_{\mathrm{opt}}\) for 20 '000 games - switch the 1st player
after every game. Choose several values of \(\epsilon_{\text {opt }}\)
from a reasonably wide interval between 0 to \(1-\) particularly,
include \(\epsilon_{\mathrm{opt}}=0\).

\textbf{Question 14}. After every 250 games during training, compute the
`test' \(M_{\text {opt }}\) and \(M_{\text {rand }}\) for your agents -
for each value of \(\epsilon_{\mathrm{opt}}\). Plot \(M_{\mathrm{opt}}\)
and \(M_{\text {rand }}\) over time. What do you observe? How can you
explain it? Expected answer: A figure showing \(M_{\mathrm{opt}}\) and
\(M_{\text {rand }}\) over time for different values of
\(\epsilon_{\mathrm{opt}}\) (caption length \(<250\) words).

\textbf{Question 15}. What are the highest values of
\(M_{\mathrm{opt}}\) and \(M_{\text {rand }}\) that you could achieve
after playing 20 '000 games?

    \hypertarget{learning-by-self-practice}{%
\subsection{Learning by self-practice}\label{learning-by-self-practice}}

    For different values of \(\epsilon \in[0,1)\), run a DQN agent against
itself for 20 ' 000 games - i.e.~both players use the same neural
network and share the same replay buffer. Important note: For one
player, you should

add states \(s_t\) and \(s_{t'}\) as \texttt{x\_t} and \texttt{x\_tp} to
the replay buffer, but for the other player, you should first swap the
opponent positions (x\_t \([:,:, 1]\) and
\(\left.x_{-} \operatorname{tp}[:,:, 1]\right)\) with the agent's own
positions (x\_t \([:,:, 0]\) and
\(\left.x_{-} \operatorname{tp}[:,:, 0]\right)\) and then add them to
the replay buffer.

\textbf{Question 16}. After every 250 games during training, compute the
`test' \(M_{\text {opt }}\) and \(M_{\text {rand }}\) for different
values of \(\epsilon \in[0,1)\). Plot \(M_{\mathrm{opt}}\) and
\(M_{\mathrm{rand}}\) over time. Does the agent learn to play Tic Tac
Toe? What is the effect of \(\epsilon\) ?\\
\emph{Expected answer}: A figure showing \(M_{\mathrm{opt}}\) and \$
M\_\{\text {rand}\}\$ over time for different values of
\(\epsilon \in[0,1)\) (caption length \(<100\) words) .

Instead of fixing \(\epsilon\), use \(\epsilon(n)\) in Equation 1 with
different values of \(n^{*}\).

\textbf{Question 17}. After every 250 games during training, compute the
`test' \(M_{\text {opt }}\) and \(M_{\text {rand }}\) for your agents.
Plot \(M_{\text {opt }}\) and \(M_{\text {rand }}\) over time. Does
decreasing \(\epsilon\) help training compared to having a fixed
\(\epsilon\) ? What is the effect of \(n^{*}\) ?\\
\emph{Expected answer}: A figure showing \(M_{\mathrm{opt}}\) and
\(M_{\text {rand }}\) over time for different values of speeds of
\(n^{*}\) (caption length \(<100\) words) .

\textbf{Question 18}. What are the highest values of
\(M_{\mathrm{opt}}\) and \(M_{\text {rand }}\) that you could achieve
after playing 20 '000 games?

\textbf{Question 19}. For three board arrangements (i.e.~states \(s\) ),
visualize \(Q\)-values of available actions (e.g.~using heat maps). Does
the result make sense? Did the agent learn the game well?\\
\emph{Expected answer}: A figure with 3 subplots of 3 different states
with \(Q\)-values shown at available actions (caption length \(<200\)
words).

    \hypertarget{comparing-q-learning-with-deep-q-learning}{%
\section{Comparing Q-Learning with Deep
Q-Learning}\label{comparing-q-learning-with-deep-q-learning}}

    We define the training time \(T_{\text {train }}\) as the number of
games an algorithm needs to play in order to reach \(80 \%\) of its
final performance according to both \(M_{\text {opt }}\) and
\(M_{\text {rand }}\).

\textbf{Question 20}. Include a table showing the best performance (the
highest \(M_{\text {opt }}\) and \(M_{\text {rand }}\) ) of
\(Q\)-Learning and DQN (both for learning from experts and for learning
by self-practice) and their corresponding training time.\\
\emph{Expected answer}: A table showing 12 values.

\textbf{Question 21}. Compare your results for DQN and Q-Learning
(answer length \(<300\) words).

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]

\end{Verbatim}
\end{tcolorbox}


    % Add a bibliography block to the postdoc
    
    
    
\end{document}
