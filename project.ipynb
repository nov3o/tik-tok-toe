{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b879e75d",
   "metadata": {},
   "source": [
    "# Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "164e57d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from tic_env import TictactoeEnv, OptimalPlayer\n",
    "\n",
    "\n",
    "st2int = lambda st: int(((st.flatten() + 1) * (3**np.arange(9))).sum())\n",
    "\n",
    "# https://xkcd.com/832/\n",
    "class Player:\n",
    "    def __init__(self, learning_rate, discount_factor, exp_rate):\n",
    "        self.states = []\n",
    "        self.states_value = {}  # state -> value\n",
    "        self.lr = learning_rate\n",
    "        self.decay_gamma = discount_factor\n",
    "        self.exp_rate = exp_rate\n",
    "        self.env = TictactoeEnv()\n",
    "                \n",
    "    def act(self, grid, symbol):\n",
    "        if np.random.uniform(0, 1) <= self.exp_rate:\n",
    "            action = self.randomMove(grid)\n",
    "        else:\n",
    "            positions = self.availablePositions(grid)\n",
    "            value_max = -999\n",
    "            if symbol == 'X':\n",
    "                symb = 1\n",
    "            elif symbol == 'O':\n",
    "                symb = -1\n",
    "            else:\n",
    "                print(\"ERROR: wrong symbol\")\n",
    "            for p in positions:\n",
    "                next_board = grid.copy()\n",
    "                next_board[p] = symb\n",
    "#                 print(next_board)\n",
    "                state_key = st2int(next_board)\n",
    "                value = 0 if self.states_value.get(state_key) is None else self.states_value[state_key]\n",
    "                if value >= value_max:\n",
    "                    value_max = value\n",
    "                    action = p\n",
    "            # print(\"{} takes action {}\".format(self.name, action))\n",
    "        return action\n",
    "    \n",
    "    def randomMove(self, grid):\n",
    "        \"\"\" Chose a random move from the available options. \"\"\"\n",
    "        return random.choice(self.availablePositions(grid))\n",
    "         \n",
    "    def availablePositions(self, grid): #nommé empty dans env\n",
    "        '''return all empty positions'''\n",
    "        return [(i // 3, i % 3) for i in range(9) if not grid[(i // 3, i % 3)]]\n",
    "    \n",
    "    def update_qtable(self, reward):\n",
    "        for state in reversed(self.states):\n",
    "            if self.states_value.get(state) is None:\n",
    "                self.states_value[state] = 0\n",
    "            self.states_value[state] += self.lr*(self.decay_gamma*reward - self.states_value[state])\n",
    "            reward = self.states_value[state]\n",
    "            \n",
    "    def addState(self, state):\n",
    "        self.states.append(st2int(state))\n",
    "            \n",
    "    def train(self, N, epsilon = 0., print_every = 100):\n",
    "        Turns = np.array(['X','O'])\n",
    "        avg_reward = 0\n",
    "        for i in range(1, N+1):\n",
    "            self.exp = i\n",
    "            self.env.reset()\n",
    "            grid, _, __ = self.env.observe()\n",
    "            Turns = Turns[::-1]\n",
    "            player_opt = OptimalPlayer(epsilon, player=Turns[0])\n",
    "                \n",
    "            for j in range(9):\n",
    "                if self.env.current_player == player_opt.player:\n",
    "                    move = player_opt.act(grid)\n",
    "                else:\n",
    "                    move = self.act(grid, Turns[1])\n",
    "\n",
    "                grid, end, winner = self.env.step(move, print_grid=False)\n",
    "                self.addState(self.env.grid.reshape(9))\n",
    "            \n",
    "                if end:\n",
    "                    if i % print_every == 0:\n",
    "                        print(\"Game n°:\", i, \"exp :\", self.exp)\n",
    "                        print('-------------------------------------------')\n",
    "                        print('Game end, winner is player ' + str(winner))\n",
    "                        print('Optimal player = ' +  Turns[0])\n",
    "                        print('Player = ' +  Turns[1])\n",
    "#                         self.env.render()\n",
    "                        print(\"AVERGAE REWARD :\", avg_reward/print_every)\n",
    "                        avg_reward = 0\n",
    "                    reward = self.env.reward(Turns[1])\n",
    "                    avg_reward += reward\n",
    "                    self.update_qtable(reward)\n",
    "                    self.env.reset()\n",
    "                    break\n",
    "                    \n",
    "    def test_policy(self, N_test, epsilon=0.):\n",
    "        Turns = np.array(['X','O'])\n",
    "        n_wins = 0\n",
    "        n_loss = 0\n",
    "        \n",
    "        for i in range(N_test):\n",
    "            self.env.reset()\n",
    "            grid, _, __ = self.env.observe()\n",
    "            Turns = Turns[::-1]\n",
    "            player_opt = OptimalPlayer(epsilon, player=Turns[0])\n",
    "                \n",
    "            for j in range(9):\n",
    "                if self.env.current_player == player_opt.player:\n",
    "                    move = player_opt.act(grid)\n",
    "                else:\n",
    "                    move = self.act(grid, Turns[1] )\n",
    "\n",
    "                grid, end, winner = self.env.step(move, print_grid=False)\n",
    "            \n",
    "                if end:\n",
    "                    if winner == Turns[1]:\n",
    "                        n_wins +=1\n",
    "                    if winner == Turns[0]:\n",
    "                        n_loss +=1\n",
    "                    self.env.reset()\n",
    "#                     print(\"Your win is \", winner == Turns[1])\n",
    "                    break\n",
    "                    \n",
    "        return (n_wins-n_loss)/N_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40252edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAIN PARAMETERS\n",
    "learning_rate = 0.05\n",
    "discount_factor = 0.99\n",
    "expl_level = 0.5\n",
    "optimal_eps_train = 0.5\n",
    "\n",
    "my_player = Player(learning_rate, discount_factor, expl_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9358a2ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game n°: 0 exp : 0\n",
      "-------------------------------------------\n",
      "Game end, winner is player None\n",
      "Optimal player = O\n",
      "Player = X\n",
      "AVERGAE REWARD : 0.0\n"
     ]
    }
   ],
   "source": [
    "#TRAIN\n",
    "games_to_train = 1000\n",
    "print_every = 1000\n",
    "my_player.train(games_to_train, optimal_eps_train, print_every)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69c99d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.016\n"
     ]
    }
   ],
   "source": [
    "#TEST\n",
    "optimal_eps_test = 1\n",
    "games_to_test = 500\n",
    "M = my_player.test_policy(games_to_test, optimal_eps_test)\n",
    "print(M)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f51443",
   "metadata": {},
   "source": [
    "##  Learning from experts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73709913",
   "metadata": {},
   "source": [
    "**Question 1**. Plot average reward for every 250 games during training – i.e. after the 50th game, plot\n",
    "the average reward of the first 250 games, after the 100th game, plot the average reward of games 51 to\n",
    "100, etc. Does the agent learn to play Tic Tac Toe? \\\n",
    "*Expected answer*: A figure of average reward over time (caption length < 50 words). Specify your choice\n",
    "of $\\epsilon$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccef2ba3",
   "metadata": {},
   "source": [
    "###  Decreasing exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7d06c7",
   "metadata": {},
   "source": [
    "###  Good experts and bad experts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fa711b",
   "metadata": {},
   "source": [
    "##  Learning by self-practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e763f84d",
   "metadata": {},
   "source": [
    "#  Deep Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5ff35855",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "\n",
    "class DQLNN(nn.Module):\n",
    "    \"\"\" DQLNN, expects input shape (3, 3, 2) \"\"\"\n",
    "    def __init__(self):\n",
    "        super(DQLNN, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(3*3*2, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, 9)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.fc3(F.relu(self.fc2(F.relu(self.fc1(x.flatten())))))\n",
    "    \n",
    "model_dql = DQLNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c741c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Player reinitializing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932a66cd",
   "metadata": {},
   "source": [
    "## Learning from experts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fc32ed",
   "metadata": {},
   "source": [
    "##      Learning by self-practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f326e3",
   "metadata": {},
   "source": [
    "# Comparing Q-Learning with Deep Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6168c94e",
   "metadata": {},
   "source": [
    "I expect DQL working better than QL"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
